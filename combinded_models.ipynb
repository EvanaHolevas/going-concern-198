{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the bankruptcy and sentiment analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# Model improvement and Evaluation \n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import regex as re\n",
    "\n",
    "# remove HTML tags from the text, remove punctuation, and convert to lowercase\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<p>', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom_standardization function as a custom object\n",
    "tf.keras.utils.get_custom_objects()[\"custom_standardization\"] = custom_standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bankruptcy and sentiment analysis models\n",
    "bankruptcy_model = tf.keras.models.load_model('model_checkpoints/model4/ckpt_200')\n",
    "\n",
    "tf.keras.utils.get_custom_objects()[\"custom_standardization\"] = custom_standardization\n",
    "going_concern_model = tf.keras.models.load_model('data/trained_models/going_concern_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have a dataset with features, bankruptcy probabilities, and sentiment analysis scores\n",
    "# X_features: Features from the first neural network\n",
    "# y_bankruptcy: Categorical probability of bankruptcy\n",
    "# sentiment_scores: Sentiment analysis scores from the second neural network\n",
    "\n",
    "bank_data = pd.read_csv(\"data/matched_data.csv\")\n",
    "numerical_only = bank_data.select_dtypes(include=['float64','int64'])\n",
    "X_bonk = numerical_only.drop(['status','COMPANY_FKEY','GOING_CONCERN','FILE_DATE','fyear'], axis=1)\n",
    "y_bonk = bank_data['status']\n",
    "\n",
    "X_go = bank_data['OPINION_TEXT1']\n",
    "y_go = bank_data['GOING_CONCERN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>current_assets</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>cost_of_goods_sold</th>\n",
       "      <th>total_long_term_debt</th>\n",
       "      <th>depreciation_and_amortization</th>\n",
       "      <th>ebit</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>inventory</th>\n",
       "      <th>total_current_liabilities</th>\n",
       "      <th>net_income</th>\n",
       "      <th>retained_earnings</th>\n",
       "      <th>total_receivables</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>market_value</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>total_operating_expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1750</td>\n",
       "      <td>913.985</td>\n",
       "      <td>1703.727</td>\n",
       "      <td>1408.071</td>\n",
       "      <td>329.802</td>\n",
       "      <td>59.296</td>\n",
       "      <td>137.016</td>\n",
       "      <td>196.312</td>\n",
       "      <td>367.711</td>\n",
       "      <td>507.274</td>\n",
       "      <td>416.010</td>\n",
       "      <td>69.826</td>\n",
       "      <td>467.485</td>\n",
       "      <td>296.489</td>\n",
       "      <td>1775.782</td>\n",
       "      <td>1049.8206</td>\n",
       "      <td>868.438</td>\n",
       "      <td>1775.782</td>\n",
       "      <td>1579.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1750</td>\n",
       "      <td>1063.272</td>\n",
       "      <td>2195.653</td>\n",
       "      <td>1662.408</td>\n",
       "      <td>669.489</td>\n",
       "      <td>80.333</td>\n",
       "      <td>142.360</td>\n",
       "      <td>222.693</td>\n",
       "      <td>412.090</td>\n",
       "      <td>599.752</td>\n",
       "      <td>473.226</td>\n",
       "      <td>67.723</td>\n",
       "      <td>486.582</td>\n",
       "      <td>324.879</td>\n",
       "      <td>2074.498</td>\n",
       "      <td>485.2897</td>\n",
       "      <td>1329.631</td>\n",
       "      <td>2074.498</td>\n",
       "      <td>1851.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1750</td>\n",
       "      <td>1033.700</td>\n",
       "      <td>2136.900</td>\n",
       "      <td>1714.500</td>\n",
       "      <td>622.200</td>\n",
       "      <td>108.600</td>\n",
       "      <td>136.600</td>\n",
       "      <td>245.200</td>\n",
       "      <td>452.600</td>\n",
       "      <td>582.900</td>\n",
       "      <td>389.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>542.400</td>\n",
       "      <td>315.400</td>\n",
       "      <td>2167.100</td>\n",
       "      <td>790.0029</td>\n",
       "      <td>1217.400</td>\n",
       "      <td>2167.100</td>\n",
       "      <td>1921.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1750</td>\n",
       "      <td>1116.900</td>\n",
       "      <td>2199.500</td>\n",
       "      <td>1581.400</td>\n",
       "      <td>564.300</td>\n",
       "      <td>113.400</td>\n",
       "      <td>142.600</td>\n",
       "      <td>256.000</td>\n",
       "      <td>453.600</td>\n",
       "      <td>632.900</td>\n",
       "      <td>402.100</td>\n",
       "      <td>72.900</td>\n",
       "      <td>616.700</td>\n",
       "      <td>297.900</td>\n",
       "      <td>2035.000</td>\n",
       "      <td>961.3080</td>\n",
       "      <td>1198.800</td>\n",
       "      <td>2035.000</td>\n",
       "      <td>1779.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>954.100</td>\n",
       "      <td>1515.000</td>\n",
       "      <td>1342.700</td>\n",
       "      <td>85.000</td>\n",
       "      <td>92.300</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>83.700</td>\n",
       "      <td>251.600</td>\n",
       "      <td>566.700</td>\n",
       "      <td>412.000</td>\n",
       "      <td>10.200</td>\n",
       "      <td>603.900</td>\n",
       "      <td>231.100</td>\n",
       "      <td>1594.300</td>\n",
       "      <td>1046.3954</td>\n",
       "      <td>669.900</td>\n",
       "      <td>1594.300</td>\n",
       "      <td>1510.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cik  current_assets  total_assets  cost_of_goods_sold  \\\n",
       "0  1750         913.985      1703.727            1408.071   \n",
       "1  1750        1063.272      2195.653            1662.408   \n",
       "2  1750        1033.700      2136.900            1714.500   \n",
       "3  1750        1116.900      2199.500            1581.400   \n",
       "4  1750         954.100      1515.000            1342.700   \n",
       "\n",
       "   total_long_term_debt  depreciation_and_amortization     ebit   ebitda  \\\n",
       "0               329.802                         59.296  137.016  196.312   \n",
       "1               669.489                         80.333  142.360  222.693   \n",
       "2               622.200                        108.600  136.600  245.200   \n",
       "3               564.300                        113.400  142.600  256.000   \n",
       "4                85.000                         92.300   -8.600   83.700   \n",
       "\n",
       "   gross_profit  inventory  total_current_liabilities  net_income  \\\n",
       "0       367.711    507.274                    416.010      69.826   \n",
       "1       412.090    599.752                    473.226      67.723   \n",
       "2       452.600    582.900                    389.000      55.000   \n",
       "3       453.600    632.900                    402.100      72.900   \n",
       "4       251.600    566.700                    412.000      10.200   \n",
       "\n",
       "   retained_earnings  total_receivables  total_revenue  market_value  \\\n",
       "0            467.485            296.489       1775.782     1049.8206   \n",
       "1            486.582            324.879       2074.498      485.2897   \n",
       "2            542.400            315.400       2167.100      790.0029   \n",
       "3            616.700            297.900       2035.000      961.3080   \n",
       "4            603.900            231.100       1594.300     1046.3954   \n",
       "\n",
       "   total_liabilities  net_sales  total_operating_expenses  \n",
       "0            868.438   1775.782                  1579.470  \n",
       "1           1329.631   2074.498                  1851.805  \n",
       "2           1217.400   2167.100                  1921.900  \n",
       "3           1198.800   2035.000                  1779.000  \n",
       "4            669.900   1594.300                  1510.600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bonk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841/841 [==============================] - 4s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = going_concern_model.predict(X_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9704408 ],\n",
       "       [0.9436229 ],\n",
       "       [0.9542874 ],\n",
       "       ...,\n",
       "       [0.9835624 ],\n",
       "       [0.9804181 ],\n",
       "       [0.81047916]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841/841 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "bonk_results = bankruptcy_model.predict(X_bonk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the categorical results in the 'categorical_results' array\n",
    "\n",
    "# Convert categorical results to binary results\n",
    "binary_results = np.argmax(bonk_results, axis=1)\n",
    "binary_results = np.expand_dims(binary_results, axis=1)\n",
    "\n",
    "print(binary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_man = list(zip(binary_results, sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = tf.keras.layers.Concatenate(axis=1)([binary_results.astype(float), sentiment_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = X_combined.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.9704408 ],\n",
       "       [0.        , 0.9436229 ],\n",
       "       [0.        , 0.9542874 ],\n",
       "       ...,\n",
       "       [1.        , 0.9835624 ],\n",
       "       [1.        , 0.9804181 ],\n",
       "       [1.        , 0.81047916]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes: (17207, 2) (17207,)\n",
      "Validation set shapes: (4302, 2) (4302,)\n",
      "Test set shapes: (5378, 2) (5378,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_combined contains the input features and y_bonk contains the binary labels\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_bonk, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the train set into train and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Train set shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hot_train = tf.one_hot(y_train,2)\n",
    "y_hot_val = tf.one_hot(y_val,2)\n",
    "y_hot_test = tf.one_hot(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([18524,  2290, 26625, ..., 12312, 12951, 17589])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bz/vm79k19966x72r5s425vw7zm0000gp/T/ipykernel_39986/91045030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Split the data into training and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# split data into training (70%), validation (15%), and testing (15%) sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bonk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.82\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2586\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2587\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     return list(\n\u001b[1;32m   2586\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2587\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m         )\n\u001b[1;32m   2589\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([18524,  2290, 26625, ..., 12312, 12951, 17589])"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # split data into training (70%), validation (15%), and testing (15%) sets \n",
    "# X_, X_test, y_, y_test = train_test_split(X_combined, y_bonk, train_size=0.8, test_size=0.15, random_state=42, shuffle=True)\n",
    "# X_train, X_validate, y_train, y_validate = train_test_split(X_, y_, train_size=0.82, test_size=0.18, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2461498177.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/bz/vm79k19966x72r5s425vw7zm0000gp/T/ipykernel_39986/2461498177.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    tf.keras.layers.Dense(6, activation='relu', input_shape=(,2)),\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create a new model to predict the impact of the going-concern report\n",
    "impact_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(6, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the impact model\n",
    "impact_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "# # Make predictions using the impact model\n",
    "# impact_probability = impact_model.predict(X_combined)\n",
    "\n",
    "# # Print the predicted impact probability\n",
    "# print('Impact probability:', impact_probability[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3280/8604 [==========>...................] - ETA: 14s - loss: 0.6274 - accuracy: 0.9637"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bz/vm79k19966x72r5s425vw7zm0000gp/T/ipykernel_39986/2705138962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpact_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hot_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hot_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[1;32m   1643\u001b[0m                             \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1373\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    641\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m     \"\"\"\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    704\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    697\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    698\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    526\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod = impact_model.fit(X_train, y_hot_train, epochs=20, batch_size=2, validation_data=(X_val, y_hot_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2858 - accuracy: 0.9570\n"
     ]
    }
   ],
   "source": [
    "resulto = impact_model.evaluate(X_test, y_hot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "673/673 [==============================] - 3s 2ms/step - loss: 0.2426 - accuracy: 0.9636\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1642 - accuracy: 0.9636\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1567 - accuracy: 0.9636\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1542 - accuracy: 0.9636\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1499 - accuracy: 0.9636\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1461 - accuracy: 0.9636\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1437 - accuracy: 0.9636\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 2s 2ms/step - loss: 0.1424 - accuracy: 0.9636\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1410 - accuracy: 0.9636\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 1s 2ms/step - loss: 0.1402 - accuracy: 0.9636\n",
      "169/169 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9570\n",
      "Test Loss: 0.16139346361160278\n",
      "Test Accuracy: 0.9570472240447998\n"
     ]
    }
   ],
   "source": [
    "# Perform feature scaling if necessary\n",
    "scaler = StandardScaler()\n",
    "X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, sentiment_train, sentiment_test = train_test_split(\n",
    "    X_features_scaled, y_bankruptcy, sentiment_scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create the model architecture\n",
    "input_features = Input(shape=(X_train.shape[1],), name='features')\n",
    "input_sentiment = Input(shape=(1,), name='sentiment')\n",
    "\n",
    "# Neural network for features\n",
    "hidden_layer = Dense(32, activation='relu')(input_features)\n",
    "output_features = Dense(1, activation='sigmoid', name='features_output')(hidden_layer)\n",
    "\n",
    "# Neural network for sentiment scores\n",
    "output_sentiment = Dense(1, activation='sigmoid', name='sentiment_output')(input_sentiment)\n",
    "\n",
    "# Concatenate the outputs of both neural networks\n",
    "concatenated = Concatenate()([output_features, output_sentiment])\n",
    "\n",
    "# Final output layer\n",
    "output = Dense(1, activation='sigmoid', name='final_output')(concatenated)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_features, input_sentiment], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, sentiment_train], y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test, sentiment_test], y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = model.get_weights()[0]  # Get the weights of the first layer\n",
    "\n",
    "# Perform analysis on feature importances\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bz/vm79k19966x72r5s425vw7zm0000gp/T/ipykernel_35566/1653923453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msorted_importances_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_importances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_importances_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfeature_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_importances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{feature_name}: {importance}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the model has been trained and evaluated on the test set\n",
    "\n",
    "# Extract feature importances from the first layer's weights\n",
    "feature_importances = np.abs(model.get_weights()[0])  # Get the weights of the first layer and take absolute values\n",
    "\n",
    "# Normalize feature importances\n",
    "normalized_importances = feature_importances / np.sum(feature_importances)\n",
    "\n",
    "# Define the feature names\n",
    "feature_names = [\n",
    "    'current_assets', 'cost_of_goods_sold', 'depreciation_and_amortization',\n",
    "    'ebitda', 'inventory', 'net_income', 'total_receivables', 'market_value',\n",
    "    'net_sales', 'total_assets', 'total_long_term_debt', 'ebit', 'gross_profit',\n",
    "    'total_current_liabilities', 'retained_earnings', 'total_revenue', 'total_liabilities',\n",
    "    'total_operating_expenses'\n",
    "]\n",
    "\n",
    "# Sort and print the feature importances in descending order\n",
    "sorted_importances_indices = np.argsort(normalized_importances)[::-1]\n",
    "for index in sorted_importances_indices:\n",
    "    feature_name = feature_names[index]\n",
    "    importance = normalized_importances[index]\n",
    "    print(f\"{feature_name}: {importance}\")\n",
    "\n",
    "# Perform further analysis on feature importances\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22614 files belonging to 2 classes.\n",
      "Using 18770 files for training.\n",
      "Found 22614 files belonging to 2 classes.\n",
      "Using 3844 files for validation.\n",
      "Found 4019 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "# create validation set\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/aligned_concerns_nostop/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.17, \n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/aligned_concerns_nostop/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.17,\n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/aligned_concerns_nostop/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data for sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import regex as re\n",
    "\n",
    "# remove HTML tags from the text, remove punctuation, and convert to lowercase\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<p>', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# convert text to numbers without labels, then adapt to the data\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Bank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import data\n",
    "bank_data = pd.read_csv(\"data/matched_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split: train, test, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>current_assets</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>cost_of_goods_sold</th>\n",
       "      <th>total_long_term_debt</th>\n",
       "      <th>depreciation_and_amortization</th>\n",
       "      <th>ebit</th>\n",
       "      <th>ebitda</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>inventory</th>\n",
       "      <th>total_current_liabilities</th>\n",
       "      <th>net_income</th>\n",
       "      <th>retained_earnings</th>\n",
       "      <th>total_receivables</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>market_value</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>net_sales</th>\n",
       "      <th>total_operating_expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1750</td>\n",
       "      <td>913.985</td>\n",
       "      <td>1703.727</td>\n",
       "      <td>1408.071</td>\n",
       "      <td>329.802</td>\n",
       "      <td>59.296</td>\n",
       "      <td>137.016</td>\n",
       "      <td>196.312</td>\n",
       "      <td>367.711</td>\n",
       "      <td>507.274</td>\n",
       "      <td>416.010</td>\n",
       "      <td>69.826</td>\n",
       "      <td>467.485</td>\n",
       "      <td>296.489</td>\n",
       "      <td>1775.782</td>\n",
       "      <td>1049.8206</td>\n",
       "      <td>868.438</td>\n",
       "      <td>1775.782</td>\n",
       "      <td>1579.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1750</td>\n",
       "      <td>1063.272</td>\n",
       "      <td>2195.653</td>\n",
       "      <td>1662.408</td>\n",
       "      <td>669.489</td>\n",
       "      <td>80.333</td>\n",
       "      <td>142.360</td>\n",
       "      <td>222.693</td>\n",
       "      <td>412.090</td>\n",
       "      <td>599.752</td>\n",
       "      <td>473.226</td>\n",
       "      <td>67.723</td>\n",
       "      <td>486.582</td>\n",
       "      <td>324.879</td>\n",
       "      <td>2074.498</td>\n",
       "      <td>485.2897</td>\n",
       "      <td>1329.631</td>\n",
       "      <td>2074.498</td>\n",
       "      <td>1851.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1750</td>\n",
       "      <td>1033.700</td>\n",
       "      <td>2136.900</td>\n",
       "      <td>1714.500</td>\n",
       "      <td>622.200</td>\n",
       "      <td>108.600</td>\n",
       "      <td>136.600</td>\n",
       "      <td>245.200</td>\n",
       "      <td>452.600</td>\n",
       "      <td>582.900</td>\n",
       "      <td>389.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>542.400</td>\n",
       "      <td>315.400</td>\n",
       "      <td>2167.100</td>\n",
       "      <td>790.0029</td>\n",
       "      <td>1217.400</td>\n",
       "      <td>2167.100</td>\n",
       "      <td>1921.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1750</td>\n",
       "      <td>1116.900</td>\n",
       "      <td>2199.500</td>\n",
       "      <td>1581.400</td>\n",
       "      <td>564.300</td>\n",
       "      <td>113.400</td>\n",
       "      <td>142.600</td>\n",
       "      <td>256.000</td>\n",
       "      <td>453.600</td>\n",
       "      <td>632.900</td>\n",
       "      <td>402.100</td>\n",
       "      <td>72.900</td>\n",
       "      <td>616.700</td>\n",
       "      <td>297.900</td>\n",
       "      <td>2035.000</td>\n",
       "      <td>961.3080</td>\n",
       "      <td>1198.800</td>\n",
       "      <td>2035.000</td>\n",
       "      <td>1779.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>954.100</td>\n",
       "      <td>1515.000</td>\n",
       "      <td>1342.700</td>\n",
       "      <td>85.000</td>\n",
       "      <td>92.300</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>83.700</td>\n",
       "      <td>251.600</td>\n",
       "      <td>566.700</td>\n",
       "      <td>412.000</td>\n",
       "      <td>10.200</td>\n",
       "      <td>603.900</td>\n",
       "      <td>231.100</td>\n",
       "      <td>1594.300</td>\n",
       "      <td>1046.3954</td>\n",
       "      <td>669.900</td>\n",
       "      <td>1594.300</td>\n",
       "      <td>1510.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cik  current_assets  total_assets  cost_of_goods_sold  \\\n",
       "0  1750         913.985      1703.727            1408.071   \n",
       "1  1750        1063.272      2195.653            1662.408   \n",
       "2  1750        1033.700      2136.900            1714.500   \n",
       "3  1750        1116.900      2199.500            1581.400   \n",
       "4  1750         954.100      1515.000            1342.700   \n",
       "\n",
       "   total_long_term_debt  depreciation_and_amortization     ebit   ebitda  \\\n",
       "0               329.802                         59.296  137.016  196.312   \n",
       "1               669.489                         80.333  142.360  222.693   \n",
       "2               622.200                        108.600  136.600  245.200   \n",
       "3               564.300                        113.400  142.600  256.000   \n",
       "4                85.000                         92.300   -8.600   83.700   \n",
       "\n",
       "   gross_profit  inventory  total_current_liabilities  net_income  \\\n",
       "0       367.711    507.274                    416.010      69.826   \n",
       "1       412.090    599.752                    473.226      67.723   \n",
       "2       452.600    582.900                    389.000      55.000   \n",
       "3       453.600    632.900                    402.100      72.900   \n",
       "4       251.600    566.700                    412.000      10.200   \n",
       "\n",
       "   retained_earnings  total_receivables  total_revenue  market_value  \\\n",
       "0            467.485            296.489       1775.782     1049.8206   \n",
       "1            486.582            324.879       2074.498      485.2897   \n",
       "2            542.400            315.400       2167.100      790.0029   \n",
       "3            616.700            297.900       2035.000      961.3080   \n",
       "4            603.900            231.100       1594.300     1046.3954   \n",
       "\n",
       "   total_liabilities  net_sales  total_operating_expenses  \n",
       "0            868.438   1775.782                  1579.470  \n",
       "1           1329.631   2074.498                  1851.805  \n",
       "2           1217.400   2167.100                  1921.900  \n",
       "3           1198.800   2035.000                  1779.000  \n",
       "4            669.900   1594.300                  1510.600  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_only = bank_data.select_dtypes(include=['float64','int64'])\n",
    "concerns = bank_data['OPINION_TEXT1','GOING_CONCERN']\n",
    "X = numerical_only.drop(['status','COMPANY_FKEY','FILE_DATE','fyear'], axis=1)\n",
    "y = numerical_only['status']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read bank data for bankruptcy model\n",
    "training = pd.read_csv('data/train_american_bankruptcy.csv')\n",
    "testing = pd.read_csv('data/test_american_bankruptcy.csv')\n",
    "validation = pd.read_csv('data/validate_american_bankruptcy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing.drop(['status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train data distribution:\n",
      " 0    16998\n",
      "1      639\n",
      "Name: status, dtype: int64\n",
      "y_validate data distribution:\n",
      " 0    3710\n",
      "1     162\n",
      "Name: status, dtype: int64\n",
      "y_test data distribution:\n",
      " 0    3866\n",
      "1     168\n",
      "Name: status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training (70%), validation (15%), and testing (15%) sets \n",
    "X_, X_test, y_, y_test = train_test_split(X, y, train_size=0.8, test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_, y_, train_size=0.82, test_size=0.18, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"y_train data distribution:\\n\", y_train.value_counts())\n",
    "print(\"y_validate data distribution:\\n\", y_validate.value_counts())\n",
    "print(\"y_test data distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "input_shape = (X.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 1s 2ms/step\n",
      "587/587 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "# X_text = preprocess_text(going_concern_report_text)\n",
    "# X_numeric = preprocess_numeric(numeric_data)\n",
    "# X_numeric = X_numeric.reshape(1, -1)  # Reshape to match expected input shape of bankruptcy model\n",
    "\n",
    "# Get the output probabilities from the two models\n",
    "y_bankruptcy = bankruptcy_model.predict(X_train)\n",
    "y_going_concern = going_concern_model.predict(raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_bank = np.round(y_bankruptcy)# Convert probabilities to binary predictions using a threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonkrupt = np.argmax(y_predict_bank, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_inputs = tf.keras.layers.concatenate([y_predict_bank, output2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bz/vm79k19966x72r5s425vw7zm0000gp/T/ipykernel_35566/349670142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatenate the two outputs into a single input vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbonkrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_going_concern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a new model to predict the impact of the going-concern report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7215\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "# Concatenate the two outputs into a single input vector\n",
    "X_combined = tf.concat([bonkrupt, y_going_concern], axis=1)\n",
    "print(X_combined)\n",
    "\n",
    "# Create a new model to predict the impact of the going-concern report\n",
    "impact_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the impact model\n",
    "impact_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Make predictions using the impact model\n",
    "impact_probability = impact_model.predict(X_combined)\n",
    "\n",
    "# Print the predicted impact probability\n",
    "print('Impact probability:', impact_probability[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.46639637e-38],\n",
       "       [0.00000000e+00, 1.00000000e+00],\n",
       "       [7.38122361e-03, 9.92618740e-01],\n",
       "       ...,\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [8.86165738e-01, 1.13834225e-01]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bankruptcy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0=no 1=yes\t\n",
    "Indicates the auditor's opinion contains an explanatory paragraph regarding the going concern assumption.\n",
    "\n",
    "Auditors include an explanitory paragraph when they conclude there is substantial doubt in a company as a 'going concern.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03828335],\n",
       "       [0.44369203],\n",
       "       [0.9488943 ],\n",
       "       ...,\n",
       "       [0.15050896],\n",
       "       [0.07220834],\n",
       "       [0.99812067]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_going_concern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
