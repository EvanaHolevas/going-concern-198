{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Stopwords Removed\n",
    "Testing sentiment analysis with the stopwords removed from the going concern reports.\n",
    "The file `going-concern-198/audit_analytics_data.ipynb` contains the model and results for training without removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (178) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "col_list = ['COMPANY_FKEY','GOING_CONCERN','OPINION_TEXT1','OPINION_TEXT2','OPINION_TEXT3']\n",
    "bank_data = pd.read_csv('data/Audit Analytics 01.2010.csv', usecols=col_list)\n",
    "# GOING_CONCERN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords & Save to folders based on label\n",
    "GOING_CONCERN: 0=no 1=yes\n",
    "\n",
    "Indicates the auditor's opinion contains an explanatory paragraph regarding the going concern assumption.\n",
    "\n",
    "Auditors include an explanitory paragraph when they conclude there is substantial doubt in a company as a 'going concern.'\n",
    "For this reason, this will be used as the label for a supervised learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evanaholevas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('data/going_concerns.csv')\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_ratio = 0.85\n",
    "test_ratio = 0.15\n",
    "\n",
    "num_samples = len(data)\n",
    "train_cutoff = int(num_samples * train_ratio)\n",
    "# test_cutoff = int((num_samples * test_ratio))\n",
    "\n",
    "train_data = data.iloc[:train_cutoff]\n",
    "test_data = data.iloc[train_cutoff:]\n",
    "# val_data = data.iloc[test_cutoff:]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dir = './data/concern_reports_nostopwords'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "# val_dir = os.path.join(output_dir, 'val')\n",
    "# if not os.path.exists(val_dir):\n",
    "#     os.makedirs(val_dir)\n",
    "\n",
    "# Convert data to the required directory structure\n",
    "def convert_data(data, dirname):\n",
    "    for label in [0, 1]:\n",
    "        label_data = data[data['GOING_CONCERN'] == label]\n",
    "        label_dir = os.path.join(dirname, str(label))\n",
    "        # create directory for label if it doesn't exist\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "        \n",
    "        \n",
    "        # create a new text file for each report titled by company id + index\n",
    "        for idx, row in label_data.iterrows():\n",
    "            filename = f\"{row['COMPANY_FKEY']}_{idx}.txt\"\n",
    "            filepath = os.path.join(label_dir, filename)\n",
    "            text = remove_stopwords(row['OPINION_TEXT1'])\n",
    "\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "\n",
    "convert_data(train_data, train_dir)\n",
    "convert_data(test_data, test_dir)\n",
    "# convert_data(val_data, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 141611\n",
      "test count: 24991\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# folder path\n",
    "dir_path2 = 'data/concern_reports_nostopwords/train'\n",
    "count2 = 0\n",
    "for root_dir, cur_dir, files in os.walk(dir_path2):\n",
    "    count2 += len(files)\n",
    "print('Train count:', count2)\n",
    "\n",
    "# folder path\n",
    "dir_path = 'data/concern_reports_nostopwords/test'\n",
    "count = 0\n",
    "for root_dir, cur_dir, files in os.walk(dir_path):\n",
    "    count += len(files)\n",
    "print('test count:', count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141611 files belonging to 2 classes.\n",
      "Using 117538 files for training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create validation set\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/concern_reports_nostopwords/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.17, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b\"Report Independent Registered Public Accounting Firm Board Directors Stockholders NCR Corporation Opinions Financial Statements Internal Control Financial Reporting audited accompanying consolidated balance sheets NCR Corporation subsidiaries ( \\xe2\\x80\\x9c Company \\xe2\\x80\\x9d ) December 31 , 2020 2019 , related consolidated statements operations , comprehensive income ( loss ) , changes stockholders \\xe2\\x80\\x99 equity cash flows three years period ended December 31 , 2020 , including related notes financial statement schedule listed index appearing Item 15 ( ) ( 2 ) ( collectively referred \\xe2\\x80\\x9c consolidated financial statements \\xe2\\x80\\x9d ) . also audited Company 's internal control financial reporting December 31 , 2020 , based criteria established Internal Control - Integrated Framework ( 2013 ) issued Committee Sponsoring Organizations Treadway Commission ( COSO ) . opinion , consolidated financial statements referred present fairly , material respects , financial position Company December 31 , 2020 2019 , results operations cash flows three years period ended December 31 , 2020 conformity accounting principles generally accepted United States America . Also opinion , Company maintained , material respects , effective internal control financial reporting December 31 , 2020 , based criteria established Internal Control - Integrated Framework ( 2013 ) issued COSO . Change Accounting Principle discussed Note 1 consolidated financial statements , Company changed manner accounts leases 2019 . Basis Opinions Company 's management responsible consolidated financial statements , maintaining effective internal control financial reporting , assessment effectiveness internal control financial reporting , included Management \\xe2\\x80\\x99 Report Internal Control Financial Reporting appearing Item 9A . responsibility express opinions Company \\xe2\\x80\\x99 consolidated financial statements Company 's internal control financial reporting based audits . public accounting firm registered Public Company Accounting Oversight Board ( United States ) ( PCAOB ) required independent respect Company accordance U.S. federal securities laws applicable rules regulations Securities Exchange Commission PCAOB . conducted audits accordance standards PCAOB . standards require plan perform audits obtain reasonable assurance whether consolidated financial statements free material misstatement , whether due error fraud , whether effective internal control financial reporting maintained material respects . audits consolidated financial statements included performing procedures assess risks material misstatement consolidated financial statements , whether due error fraud , performing procedures respond risks . procedures included examining , test basis , evidence regarding amounts disclosures consolidated financial statements . audits also included evaluating accounting principles used significant estimates made management , well evaluating overall presentation consolidated financial statements . audit internal control financial reporting included obtaining understanding internal control financial reporting , assessing risk material weakness exists , testing evaluating design operating effectiveness internal control based assessed risk . audits also included performing procedures considered necessary circumstances . believe audits provide reasonable basis opinions . Definition Limitations Internal Control Financial Reporting company \\xe2\\x80\\x99 internal control financial reporting process designed provide reasonable assurance regarding reliability financial reporting preparation financial statements external purposes accordance generally accepted accounting principles . company \\xe2\\x80\\x99 internal control financial reporting includes policies procedures ( ) pertain maintenance records , reasonable detail , accurately fairly reflect transactions dispositions assets company ; ( ii ) provide reasonable assurance transactions recorded necessary permit preparation financial statements accordance generally accepted accounting principles , receipts expenditures company made accordance authorizations management directors company ; ( iii ) provide reasonable assurance regarding prevention timely detection unauthorized acquisition , use , disposition company \\xe2\\x80\\x99 assets could material effect financial statements . inherent limitations , internal control financial reporting may prevent detect misstatements . Also , projections evaluation effectiveness future periods subject risk controls may become inadequate changes conditions , degree compliance policies procedures may deteriorate . Critical Audit Matters critical audit matters communicated matters arising current period audit consolidated financial statements communicated required communicated audit committee ( ) relate accounts disclosures material consolidated financial statements ( ii ) involved especially challenging , subjective , complex judgments . communication critical audit matters alter way opinion consolidated financial statements , taken whole , , communicating critical audit matters , providing separate opinions critical audit matters accounts disclosures relate . Valuation Hospitality Reporting Unit used Goodwill Impairment Analysis described Notes 1 2 consolidated financial statements , Company \\xe2\\x80\\x99 consolidated goodwill balance $ 2,837 million December 31 , 2020 , goodwill associated Hospitality reporting unit $ 381 million . Goodwill tested reporting unit level impairment annual basis fourth quarter frequently certain events occur indicating carrying amount goodwill may impaired . early 2020 , significant market volatility driven COVID-19 pandemic drove uncertainty around Company \\xe2\\x80\\x99 full year revenue operating income expectations . result , management determined indication carrying value net assets assigned Hospitality reporting unit may recoverable . quantitative assessments Hospitality reporting unit completed March 31 , 2020 fourth quarter 2020 , fair value estimated using weighted methodology considering output income market approaches . income approach incorporates use discounted cash flow ( DCF ) analysis . number significant assumptions estimates involved application discounted cash flow model forecast operating cash flows , including revenue growth , operating income margin discount rate . market approach performed using Guideline Public Companies ( GPC ) method based earnings multiple data peer companies . principal considerations determination performing procedures relating valuation Hospitality reporting unit used goodwill impairment analysis critical audit matter significant judgment management developing fair value measurements Hospitality reporting unit , turn led high degree auditor judgment , subjectivity effort performing procedures evaluating audit evidence related management \\xe2\\x80\\x99 significant assumptions related revenue growth , operating income margin discount rate . addition , audit effort involved use professionals specialized skill knowledge . Addressing matter involved performing procedures evaluating audit evidence connection forming overall opinion consolidated financial statements . procedures included testing effectiveness controls relating management \\xe2\\x80\\x99 goodwill impairment assessment , including controls valuation Company \\xe2\\x80\\x99 Hospitality reporting unit . procedures also included , among others , ( ) testing management \\xe2\\x80\\x99 process developing fair value measurements ; ( ii ) evaluating appropriateness discounted cash flow model ; ( iii ) testing completeness , accuracy relevance underlying data used model ; ( iv ) evaluating significant assumptions used management related revenue growth , operating income margin discount rate . Evaluating management \\xe2\\x80\\x99 assumptions related revenue growth operating income margin involved evaluating whether assumptions used reasonable considering ( ) current past performance reporting unit , ( ii ) consistency external market data , ( iii ) whether assumptions consistent evidence obtained areas audit . Professionals specialized skill knowledge used assist evaluation Company \\xe2\\x80\\x99 ( ) discounted cash flow model discount rate assumption ; ( ii ) market approach peer companies market multiple data used management ; ( iii ) weighting two approaches . Realizability U.S. Net Deferred Tax Assets described Note 6 consolidated financial statements , Company \\xe2\\x80\\x99 U.S. net deferred tax assets ( without valuation allowances ) $ 469 million December 31 , 2020 . three-year period ended December 31 , 2020 , U.S. cumulative net loss continuing operations income taxes , adjusted permanent differences , generally considered negative indicator Company \\xe2\\x80\\x99 ability realize benefits assets . Management evaluated realizability U.S. net deferred tax assets weighing positive negative evidence , including Company \\xe2\\x80\\x99 history U.S. pre-tax income adjusted permanent differences , impact COVID-19 pandemic Company \\xe2\\x80\\x99 U.S. results 2020 near-term , projected U.S. taxable income , length time Company \\xe2\\x80\\x99 deferred tax assets relating net operating losses , general basket foreign tax credits , interest limitation carryforward , research development credits variety temporary differences may realized . assessment , realization related benefits determined management likely . principal considerations determination performing procedures relating realizability U.S. net deferred tax assets critical audit matter significant judgment management assessing whether likely Company realize U.S. net deferred tax assets , turn led high degree auditor judgment , subjectivity effort performing procedures evaluating audit evidence related management \\xe2\\x80\\x99 significant assumptions related ( ) projected U.S. taxable income ( ii ) length time Company \\xe2\\x80\\x99 deferred tax assets relating general basket foreign tax credits research development credits may realized . Also , audit effort involved use professionals specialized skill knowledge . Addressing matter involved performing procedures evaluating audit evidence connection forming overall opinion consolidated financial statements . procedures included testing effectiveness controls relating assessing realizability U.S. net deferred tax assets . procedures also included , among others , ( ) testing management \\xe2\\x80\\x99 process assessing realizability U.S. net deferred tax assets estimating projected U.S. taxable income ; ( ii ) testing completeness , accuracy relevance underlying data used assessment realizability U.S. deferred tax assets ; ( iii ) evaluating significant assumptions used management related projected U.S. taxable income length time Company \\xe2\\x80\\x99 deferred tax assets relating general basket foreign tax credits research development credits may realized ; ( iv ) evaluating positive negative evidence used management . Evaluating management \\xe2\\x80\\x99 assumptions related projected U.S. taxable income length time deferred tax assets relating general basket foreign tax credits research development credits may realized involved evaluating whether assumptions used reasonable considering ( ) current past performance Company \\xe2\\x80\\x99 U.S. operations ; ( ii ) whether assumptions consistent evidence obtained areas audit . Professionals specialized skills knowledge utilized assist evaluation Company \\xe2\\x80\\x99 assessment length time deferred tax assets relating general basket foreign tax credits research development credits may realized . /s/ PricewaterhouseCoopers LLP Atlanta , Georgia February 26 , 2021 served Company \\xe2\\x80\\x99 auditor since 1993 .\"\n",
      "Label 0\n",
      "Review b\"REPORT INDEPENDENT REGISTERED PUBLIC ACCOUNTING FIRM Board Directors Stockholders Comstock Resources , Inc. Opinion Financial Statements audited accompanying consolidated balance sheets Comstock Resources , Inc. subsidiaries ( Company ) December 31 , 2018 2019 , related consolidated statements operations , stockholders \\xe2\\x80\\x99 equity , cash flows year ended December 31 , 2017 ( Predecessor ) , period January 1 , 2018 August 13 , 2018 ( Predecessor ) , period August 14 , 2018 December 31 , 2018 ( Successor ) , year ended December 31 , 2019 ( Successor ) , related notes ( collectively referred \\xe2\\x80\\x9c consolidated financial statements \\xe2\\x80\\x9c ) . opinion , consolidated financial statements present fairly , material respects , financial position Company December 31 , 2018 2019 , results operations cash flows year ended December 31 , 2017 ( Predecessor ) , period January 1 , 2018 August 13 , 2018 ( Predecessor ) , period August 14 , 2018 December 31 , 2018 ( Successor ) , year ended December 31 , 2019 ( Successor ) , conformity U.S. generally accepted accounting principles . also audited , accordance standards Public Company Accounting Oversight Board ( United States ) ( PCAOB ) , Company \\xe2\\x80\\x98 internal control financial reporting December 31 , 2019 , based criteria established Internal Control-Integrated Framework issued Committee Sponsoring Organizations Treadway Commission ( 2013 framework ) report dated March 2 , 2020 expressed unqualified opinion thereon . Basis Opinion financial statements responsibility Company \\xe2\\x80\\x98 management . responsibility express opinion Company \\xe2\\x80\\x98 financial statements based audits . public accounting firm registered PCAOB required independent respect Company accordance U.S. federal securities laws applicable rules regulations Securities Exchange Commission PCAOB . conducted audits accordance standards PCAOB . standards require plan perform audit obtain reasonable assurance whether financial statements free material misstatement , whether due error fraud . audits included performing procedures assess risks material misstatement financial statements , whether due error fraud , performing procedures respond risks . procedures included examining , test basis , evidence regarding amounts disclosures financial statements . audits also included evaluating accounting principles used significant estimates made management , well evaluating overall presentation financial statements . believe audits provide reasonable basis opinion . /s/ ERNST & YOUNG LLP served Company 's auditor since 2003 . Dallas , Texas March 2 , 2020\"\n",
      "Label 0\n",
      "Review b'REPORT INDEPENDENT REGISTERED PUBLIC ACCOUNTING FIRM Board Directors Shareholders Astec Industries , Inc. < p > audited accompanying consolidated balance sheets Astec Industries , Inc. December 31 , 2013 2012 related consolidated statements income , comprehensive income , equity , cash flows three years period ended December 31 , 2013 . financial statements responsibility Company \\xe2\\x80\\x99 management . responsibility express opinion financial statements based audits . conducted audits accordance standards Public Company Accounting Oversight Board ( United States ) . standards require plan perform audit obtain reasonable assurance whether financial statements free material misstatement . audit includes examining , test basis , evidence supporting amounts disclosures financial statements . audit also includes assessing accounting principles used significant estimates made management , well evaluating overall financial statement presentation . believe audits provide reasonable basis opinion. < p > opinion , financial statements referred present fairly , material respects , consolidated financial position Astec Industries , Inc. December 31 , 2013 2012 , consolidated results operations cash flows three years period ended December 31 , 2013 , conformity U.S. generally accepted accounting principles. < p > also audited , accordance standards Public Company Accounting Oversight Board ( United States ) , Astec Industries , Inc. \\xe2\\x80\\x99 internal control financial reporting December 31 , 2013 , based criteria established Internal Control-Integrated Framework issued Committee Sponsoring Organizations Treadway Commission ( 1992 framework ) report dated March 3 , 2014 expressed unqualified opinion thereon . /s/ Ernst & Young LLP Chattanooga , Tennessee March 3 , 2014'\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141611 files belonging to 2 classes.\n",
      "Using 24073 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/going_concern_reports/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.17,\n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24991 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'data/going_concern_reports/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import regex as re\n",
    "\n",
    "# remove HTML tags from the text, remove punctuation, and convert to lowercase\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<p>', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# convert text to numbers without labels, then adapt to the data\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the preprocessed data\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 --->  â€¢determining\n",
      " 313 --->  met\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[9999])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[2000])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text vectorization layer\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "# define the model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy loss function for binary classification of text\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.00008)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0236 - binary_accuracy: 0.9919 - val_loss: 1.0583 - val_binary_accuracy: 0.6131\n",
      "Epoch 2/20\n",
      "3674/3674 [==============================] - 24s 6ms/step - loss: 0.0228 - binary_accuracy: 0.9923 - val_loss: 1.0656 - val_binary_accuracy: 0.6172\n",
      "Epoch 3/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0219 - binary_accuracy: 0.9927 - val_loss: 1.0069 - val_binary_accuracy: 0.6373\n",
      "Epoch 4/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0212 - binary_accuracy: 0.9930 - val_loss: 0.8731 - val_binary_accuracy: 0.6817\n",
      "Epoch 5/20\n",
      "3674/3674 [==============================] - 24s 7ms/step - loss: 0.0205 - binary_accuracy: 0.9932 - val_loss: 0.7873 - val_binary_accuracy: 0.7149\n",
      "Epoch 6/20\n",
      "3674/3674 [==============================] - 24s 6ms/step - loss: 0.0200 - binary_accuracy: 0.9935 - val_loss: 0.6921 - val_binary_accuracy: 0.7511\n",
      "Epoch 7/20\n",
      "3674/3674 [==============================] - 24s 7ms/step - loss: 0.0194 - binary_accuracy: 0.9935 - val_loss: 0.5815 - val_binary_accuracy: 0.7946\n",
      "Epoch 8/20\n",
      "3674/3674 [==============================] - 24s 7ms/step - loss: 0.0192 - binary_accuracy: 0.9936 - val_loss: 0.4806 - val_binary_accuracy: 0.8327\n",
      "Epoch 9/20\n",
      "3674/3674 [==============================] - 24s 7ms/step - loss: 0.0186 - binary_accuracy: 0.9937 - val_loss: 0.3804 - val_binary_accuracy: 0.8694\n",
      "Epoch 10/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0182 - binary_accuracy: 0.9939 - val_loss: 0.3346 - val_binary_accuracy: 0.8839\n",
      "Epoch 11/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0178 - binary_accuracy: 0.9941 - val_loss: 0.2519 - val_binary_accuracy: 0.9121\n",
      "Epoch 12/20\n",
      "3674/3674 [==============================] - 24s 7ms/step - loss: 0.0175 - binary_accuracy: 0.9942 - val_loss: 0.2255 - val_binary_accuracy: 0.9217\n",
      "Epoch 13/20\n",
      "3674/3674 [==============================] - 26s 7ms/step - loss: 0.0172 - binary_accuracy: 0.9942 - val_loss: 0.1730 - val_binary_accuracy: 0.9397\n",
      "Epoch 14/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0171 - binary_accuracy: 0.9943 - val_loss: 0.1519 - val_binary_accuracy: 0.9420\n",
      "Epoch 15/20\n",
      "3674/3674 [==============================] - 25s 7ms/step - loss: 0.0168 - binary_accuracy: 0.9945 - val_loss: 0.1528 - val_binary_accuracy: 0.9392\n",
      "Epoch 16/20\n",
      "3674/3674 [==============================] - 28s 8ms/step - loss: 0.0164 - binary_accuracy: 0.9945 - val_loss: 0.1596 - val_binary_accuracy: 0.9339\n",
      "Epoch 17/20\n",
      "3674/3674 [==============================] - 29s 8ms/step - loss: 0.0162 - binary_accuracy: 0.9946 - val_loss: 0.1841 - val_binary_accuracy: 0.9184\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# epochs = 10\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=epochs)\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=20,\n",
    "          batch_size=32,\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=3)]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 4s 5ms/step - loss: 0.1935 - binary_accuracy: 0.9165\n",
      "Loss:  0.19345645606517792\n",
      "Accuracy:  0.9165299534797668\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 5s 6ms/step - loss: 0.1935 - accuracy: 0.9272\n",
      "0.9272137880325317\n"
     ]
    }
   ],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Testing with the raw input `raw_test_ds`\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24991 files belonging to 2 classes.\n",
      "781/781 [==============================] - 4s 5ms/step\n",
      "Precision: 0.1696085955487337\n",
      "Recall: 0.10783117833617956\n",
      "F1 score: 0.13184190902311707\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set up the test data\n",
    "test_dir = 'data/concern_reports_nostopwords/test'\n",
    "batch_size = 32\n",
    "test_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = export_model.predict(raw_test_ds)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = []\n",
    "for _, labels in test_data:\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "# Convert probabilities to binary labels\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate precision, recall, and f1 score\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
